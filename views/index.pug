extends layout

block title
  title ReCoRD Dataset

block description
  meta(name='description', content='The Reading comprehension with Common-sense Reasoning Dataset (ReCoRD) is a new reading comprehension dataset requiring common-sense reasoning. It consists of queries automatically generated from a set of news articles, where the answer to every query is a segment of text, or span, from a summarizing passage of the corresponding news article. With 120,000+ query-answer pairs on 50,000+ articles, REACHER is the largest reading comprehension dataset which requires common-sense reasoning.')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin record_1_model_display(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th EM
      th F1
    - var human_em = 90.831
    - var human_f1 = 91.452
    - var largest_em = Math.max.apply(null, group.map(function (model) { return model.em; }))
    - var largest_f1 = Math.max.apply(null, group.map(function (model) { return model.f1; }))
      tr.human-row
        td
        td
          | Human Performance
          p.institution Johns Hopkins University
          a(href="http://arxiv.org/abs/1606.05250") (Zhang et al. '18)
        td #{human_em}
        td #{human_f1}
    each model in group
      tr
        if is_test
          td 
            p #{model.rank}
            span.date.label.label-default #{moment.unix(model.date).format('MMM DD, YYYY')}
        td(style="word-break:break-word;")
          | #{model.model_name}
          p.institution #{model.institution}
          if model.link
            a.link(href=model.link) #{model.link}
        td
          if model.em == largest_em
            b #{model.em.toPrecision(5)}
          else
            | #{model.em.toPrecision(5)}
        td
          if model.f1 == largest_f1
            b #{model.f1.toPrecision(5)}
          else
            | #{model.f1.toPrecision(5)}

block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is Re
                  span.record Co
                  |RD?
              p 
                span
                  b Re
                  | ading 
                  b Co
                  |mprehension with 
                  b Co
                  | mmon-sense  
                  b R
                  | easoning 
                  b D
                  | ataset (Re
                  span.record Co
                |RD)  
                | is a new reading comprehension dataset requiring common-sense reasoning. 
                | Re
                span.record Co
                | RD consists of queries automatically generated from CNN/Daily Mail news articles; 
                | the answer to every query is a segment of text, or 
                i span
                | , from a summarizing passage of the corresponding news article.
                | The goal of Re
                span.record Co
                | RD is to evaluate the ability of machine to do common-sense reasoning in reading comprehension. 
                | Re
                span.record Co
                | RD is pronounced as /ˈɹɛk.ɚd/.
              hr
              p
                b Re
                  span.record Co
                  | RD 
                | contains 120,000+ queries from 60,000+ news articles.
                | Each query has been validated by crowdworkers.
                | Unlike existing reading comprehension datasets, Re
                span.record Co
                | RD contains a large portion (80%) of queries requiring common-sense
                | reasoning, therefore showing a significant gap between human and machine reading comprehension. 
              // a.btn.actionBtn(href="/explore/1.1/dev/") Explore SQuAD and model predictions
              a.btn.actionBtn(href="http://arxiv.org/abs/1806.03822") Re
                span.record Co
                | RD paper (Zhang et al. '18)
              hr
              p
                | Browse the examples in Re
                span.record Co
                | RD in a friendly way:
              a.btn.actionBtn(href="/examples/0.html") Browse Re
                span.record Co
                | RD
              .infoHeadline
                h2 Getting Started
              p
                | We've built a few resources to help you get started with the dataset.
              p 
                | Download a copy of the dataset in JSON format:
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="/dataset/train-v2.0.json", download)
                      | Training Set (40 MB)
                  li
                    a.btn.actionBtn.inverseBtn(href="/dataset/dev-v2.0.json", download)
                      | Dev Set (4 MB)
              p To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use 
                code
                  | python evaluate.py &lt;path_to_dev&gt; &lt;path_to_predictions&gt;
                |.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/", download)
                      | Evaluation Script
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/bundles/0x8731effab84f41b7b874a070e40f61e2/", download)
                      | Sample Prediction File
              p Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:
              a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/")
                | Submission Tutorial
              .infoHeadline
                h2 License
              p Re
                span.record Co
                | RD contains passages from two domains. 
                | We make them public under the following licenses:
                ul
                  li Passages and queries collected from the 
                    a(href="https://github.com/deepmind/rc-data") CNN/Daily Mail dataset
                    | are under 
                    a(href="https://github.com/deepmind/rc-data/blob/master/LICENSE") Apache 
                    | license.
                  li Passages and queries crawled from 
                    a(href="https://archive.org/") Internet Archive 
                    | are subject to the 
                    a(href="https://archive.org/about/terms.php") Term of Use 
                    | .
              .infoHeadline
                h2 Have Questions?
              p 
                | Ask us questions at our   
                a(href="https://groups.google.com/forum/#!forum/squad-stanford-qa") google group
                |  or at 
                a(href="mailto:zsheng2@jhu.edu") zsheng2@jhu.edu
                | .
              .infoHeadline
                h2 Acknowledgements
              p  
                | We thank the 
                a(href="https://rajpurkar.github.io/SQuAD-explorer/") SQuAD team 
                | for allowing us to use their code and templates for generating this website.
            .infoSubheadline
              include includes/tweet
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Leaderboard
              +record_1_model_display(baselines, true)
