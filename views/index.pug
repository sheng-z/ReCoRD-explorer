extends layout

block title
  title ReCoRD

block description
  meta(name='description', content='The Reading comprehension with Commonsense Reasoning Dataset (ReCoRD) is a new reading comprehension dataset requiring commonsense reasoning. It consists of queries automatically generated from a set of news articles, where the answer to every query is a segment of text, or span, from a summarizing passage of the corresponding news article. With 120,000+ query-answer pairs on 50,000+ articles, REACHER is the largest reading comprehension dataset which requires commonsense reasoning.')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin record_1_model_display(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th EM
      th F1
    - var human_em = 91.31
    - var human_f1 = 91.69
    - var largest_em = Math.max.apply(null, group.map(function (model) { return model.em; }))
    - var largest_f1 = Math.max.apply(null, group.map(function (model) { return model.f1; }))
      tr.human-row
        td
        td
          | Human Performance
          p.institution Johns Hopkins University
          a(href="https://arxiv.org/pdf/1810.12885.pdf") (Zhang et al. '18)
        td #{human_em}
        td #{human_f1}
    each model in group
      tr
        if is_test
          td
            p #{model.rank}
            span.date.label.label-default #{moment.unix(model.date).format('MMM DD, YYYY')}
        td(style="word-break:break-word;")
          | #{model.model_name}
          p.institution #{model.institution}
          if model.link
            a.link(href=model.link) #{model.link}
        td
          if model.em == largest_em
            b #{model.em.toPrecision(4)}
          else
            | #{model.em.toPrecision(4)}
        td
          if model.f1 == largest_f1
            b #{model.f1.toPrecision(4)}
          else
            | #{model.f1.toPrecision(4)}

block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is Re
                  span.record Co
                  |RD?
              p
                span
                  b Re
                  | ading 
                  b Co
                  |mprehension with 
                  b Co
                  | mmonsense 
                  b R
                  | easoning 
                  b D
                  | ataset (Re
                  span.record Co
                |RD)
                | is a large-scale reading comprehension dataset which requires commonsense reasoning. 
                | Re
                span.record Co
                | RD consists of queries automatically generated from CNN/Daily Mail news articles;
                | the answer to each query is a text span from a summarizing passage of the corresponding news.
                | The goal of Re
                span.record Co
                | RD is to evaluate a machine's ability of commonsense reasoning in reading comprehension.
                | Re
                span.record Co
                | RD is pronounced as [ˈrɛkərd].
              hr
              p
                b Re
                  span.record Co
                  | RD 
                | contains 120,000+ queries from 70,000+ news articles.
                | Each query has been validated by crowdworkers.
                | Unlike existing reading comprehension datasets, Re
                span.record Co
                | RD contains a large portion of queries requiring commonsense
                | reasoning, thus presenting a good challenge for future research to bridge the gap between human and machine commonsense reading comprehension.
              a.btn.actionBtn(href="https://arxiv.org/pdf/1810.12885.pdf") Re
                span.record Co
                | RD paper (Zhang et al. '18)
              hr
              p
                | Browse examples in Re
                span.record Co
                | RD in a friendly way:
              a.btn.actionBtn(href="/examples/000.html") Browse Re
                span.record Co
                | RD
              .infoHeadline
                h2 Getting Started
              p
                | We've built a few resources to help you get started with the dataset.
              p
                | Download a copy of the dataset in JSON format:
              ul.list-unstyled
                li
                  a.btn.actionBtn.inverseBtn(href="https://drive.google.com/file/d/1PoHmphyH79pETNws8kU2OwuerU7SWLHj/view?usp=sharing")
                    | Training Set (217 MB)
                li
                  a.btn.actionBtn.inverseBtn(href="https://drive.google.com/file/d/1WNaxBpXEGgPbymTzyN249P4ub-uU5dkO/view?usp=sharing")
                    | Dev Set (24 MB)
              P Read the following Readme to get familiar with the data structure.
              ul.list-unstyled
                li
                  a.btn.actionBtn.inverseBtn(href="/dataset-readme.txt")
                    | Readme
              p To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use 
                code
                  | python evaluate.py &lt;path_to_dev&gt; &lt;path_to_predictions&gt;
                |.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="/evaluation.py")
                      | Evaluation Script
                  li
                    a.btn.actionBtn.inverseBtn(href="/")
                      | Sample Prediction File
              p Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:
                a.btn.actionBtn.inverseBtn(href="/")
                  | Submission Tutorial
              .infoHeadline
                h2 License
              p Re
                span.record Co
                | RD contains passages from two domains.
                | We make them public under the following licenses:
                ul
                  li Passages and queries collected from the 
                    a(href="https://github.com/deepmind/rc-data") CNN/Daily Mail dataset 
                    | are under 
                    a(href="https://github.com/deepmind/rc-data/blob/master/LICENSE") Apache 
                    | license. 
                  li Passages and queries crawled from 
                    a(href="https://archive.org/") Internet Archive 
                    | are subject to the 
                    a(href="https://archive.org/about/terms.php") Term of Use
                    | .
              .infoHeadline
                h2 Have Questions?
              p
                | Ask us questions at our 
                a(href="https://groups.google.com/forum/#!forum/record-qa") google group
                |  or at
                a(href="mailto:zsheng2@jhu.edu") zsheng2@jhu.edu
                | .
              .infoHeadline
                h2 Acknowledgements
              p
                | We thank the 
                a(href="https://rajpurkar.github.io/SQuAD-explorer/") SQuAD team 
                | for allowing us to use their code and templates for generating this website.
            .infoSubheadline
              include includes/tweet
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Leaderboard
              +record_1_model_display(baselines, true)
