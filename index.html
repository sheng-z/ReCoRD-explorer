<!DOCTYPE html><!--Author: Sheng Zhang 2018--><html><head><meta charset="utf-8"><title>ReCoRD</title><meta name="description" content="The Reading comprehension with Commonsense Reasoning Dataset (ReCoRD) is a new reading comprehension dataset requiring commonsense reasoning. It consists of queries automatically generated from a set of news articles, where the answer to every query is a text span, from a summarizing passage of the corresponding news article."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/logo.png"><link rel="image_src" type="image/png" href="/ReCoRD-explorer/logo.png"><link rel="shortcut icon" href="/ReCoRD-explorer/favicon.ico" type="image/x-icon"><link rel="icon" href="/ReCoRD-explorer/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/ReCoRD-explorer/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/ReCoRD-explorer/stylesheets/layout.css"><link rel="stylesheet" href="/ReCoRD-explorer/stylesheets/index.css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/ReCoRD-explorer/javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/ReCoRD-explorer/">Home</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/ReCoRD-explorer/">Re<span class="record">Co</span>RD</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle"> <b>Re</b><span class="record-title">Co</span><b>RD</b></h1><h2 id="appSubtitle">Reading Comprehension with Commonsense Reasoning Dataset</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-5"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>News</h2></div><p><ul class="ulNews"><li class="liNews"><span class="Newsdate">07/15/2019</span><a href="https://super.gluebenchmark.com/tasks"> SuperGLUE</a> added Re<span class="record">Co</span>RD in its evaluation suite.</li><li class="liNews"><span class="Newsdate">03/17/2019</span> Re<span class="record">Co</span>RD is now a shared task in the <a href="https://coinnlp.github.io/">COIN</a> workshop at <a href="https://www.emnlp-ijcnlp2019.org/program/workshops/">EMNLP 2019</a>.</li></ul></p><div class="infoHeadline"><h2>What is Re<span class="record">Co</span>RD?</h2></div><p><span><b>Re</b>ading <b>Co</b>mprehension with <b>Co</b>mmonsense <b>R</b>easoning <b>D</b>ataset (Re<span class="record">Co</span></span>RD)
is a large-scale reading comprehension dataset which requires commonsense reasoning. 
Re<span class="record">Co</span>RD consists of queries automatically generated from CNN/Daily Mail news articles;
the answer to each query is a text span from a summarizing passage of the corresponding news.
The goal of Re<span class="record">Co</span>RD is to evaluate a machine's ability of commonsense reasoning in reading comprehension.
Re<span class="record">Co</span>RD is pronounced as [ˈrɛkərd].</p><hr><p><b>Re<span class="record">Co</span>RD </b>contains 120,000+ queries from 70,000+ news articles.
Each query has been validated by crowdworkers.
Unlike existing reading comprehension datasets, Re<span class="record">Co</span>RD contains a large portion of queries requiring commonsense
reasoning, thus presenting a good challenge for future research to bridge the gap between human and machine commonsense reading comprehension.</p><a class="btn actionBtn" href="https://arxiv.org/pdf/1810.12885.pdf">Re<span class="record">Co</span>RD paper (Zhang et al. '18)</a><hr><p>Browse examples in Re<span class="record">Co</span>RD in a friendly way:</p><a class="btn actionBtn" href="/ReCoRD-explorer/examples/000.html">Browse Re<span class="record">Co</span>RD</a><div class="infoHeadline"><h2>Getting Started</h2></div><p>We've built a few resources to help you get started with the dataset.</p><p>Download a copy of the dataset in JSON format:</p><ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://drive.google.com/file/d/1PoHmphyH79pETNws8kU2OwuerU7SWLHj/view?usp=sharing">Training Set (217 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="https://drive.google.com/file/d/1WNaxBpXEGgPbymTzyN249P4ub-uU5dkO/view?usp=sharing">Dev Set (24 MB)</a></li></ul><P>Read the following Readme to get familiar with the data structure.</P><ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="/ReCoRD-explorer/dataset-readme.txt">Readme</a></li></ul><p>To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use <code>python evaluate.py &lt;path_to_dev&gt; &lt;path_to_predictions&gt;</code>.<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="/ReCoRD-explorer/evaluation.py">Evaluation Script</a></li><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/bundles/0x082bda8870fc43f99c4aedb93bfa146e/">Sample Prediction File</a></li></ul></p><p>Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:<a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/worksheets/0x683ccf06dbe34c0384465f861020f917/">Submission Tutorial</a></p><div class="infoHeadline"><h2>License</h2></div><p>Re<span class="record">Co</span>RD contains passages from two domains.
We make them public under the following licenses:<ul><li>Passages and queries collected from the <a href="https://github.com/deepmind/rc-data">CNN/Daily Mail dataset </a>are under <a href="https://github.com/deepmind/rc-data/blob/master/LICENSE">Apache </a>license.</li><li>Passages and queries crawled from <a href="https://archive.org/">Internet Archive </a>are subject to the <a href="https://archive.org/about/terms.php">Term of Use</a>.</li></ul></p><div class="infoHeadline"><h2>Have Questions?</h2></div><p>Ask us questions at our <a href="https://groups.google.com/forum/#!forum/record-qa">google group </a> or at <a href="mailto:zsheng2@jhu.edu">zsheng2@jhu.edu</a>.</p><div class="infoHeadline"><h2>Acknowledgements</h2></div><p>We thank the <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD team </a>for allowing us to use their code and templates for generating this website.</p></div><div class="infoSubheadline"><a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-size="large" data-text="The ReCoRD Dataset - 120,000+ queries for reading comprehension with common-sense reasoning" data-url="https://sheng-z.github.io/ReCoRD/" data-via="sheng_zh" data-hashtags="ReCORD" data-related="sheng_zh" data-show-count="false">Tweet</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div></div></div><div class="col-md-7"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Leaderboard</h2></div><table class="table performanceTable"><tr><th>Rank</th><th>Model</th><th>EM</th><th>F1</th></tr><tr class="human-row"><td></td><td>Human Performance<p class="institution">Johns Hopkins University</p><a href="https://arxiv.org/pdf/1810.12885.pdf">(Zhang et al. '18)</a></td><td>91.31</td><td>91.69</td></tr><tr><td><p>1</p><span class="date label label-default">Jul 20, 2019</span></td><td style="word-break:break-word;">XLNet + MTL + Verifier (ensemble)<p class="institution">PingAn Smart Health &amp; SJTU</p></td><td><b>83.09</b></td><td><b>83.74</b></td></tr><tr><td><p>2</p><span class="date label label-default">Jul 09, 2019</span></td><td style="word-break:break-word;">CSRLM (single model)<p class="institution">Anonymous</p></td><td>81.78</td><td>82.58</td></tr><tr><td><p>3</p><span class="date label label-default">Jul 24, 2019</span></td><td style="word-break:break-word;">{SKG-NET} (single model)<p class="institution">Anonymous</p></td><td>79.48</td><td>80.04</td></tr><tr><td><p>4</p><span class="date label label-default">Jan 11, 2019</span></td><td style="word-break:break-word;">KT-NET (single model)<p class="institution">Baidu NLP</p></td><td>71.60</td><td>73.62</td></tr><tr><td><p>4</p><span class="date label label-default">May 16, 2019</span></td><td style="word-break:break-word;">SKG-BERT (single model)<p class="institution">Anonymous</p></td><td>72.24</td><td>72.78</td></tr><tr><td><p>5</p><span class="date label label-default">Nov 29, 2018</span></td><td style="word-break:break-word;">DCReader+BERT (single model)<p class="institution">Anonymous</p></td><td>69.49</td><td>71.14</td></tr><tr><td><p>6</p><span class="date label label-default">Nov 16, 2018</span></td><td style="word-break:break-word;">BERT-Base (single model)<p class="institution">JHU [modification of the Google AI implementation]</p><a class="link" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></td><td>54.04</td><td>56.07</td></tr><tr><td><p>7</p><span class="date label label-default">Oct 25, 2018</span></td><td style="word-break:break-word;">DocumentQA w/ ELMo (single model)<p class="institution">JHU [modification of the AllenNLP implementation]</p><a class="link" href="https://arxiv.org/pdf/1710.10723.pdf">https://arxiv.org/pdf/1710.10723.pdf</a></td><td>45.44</td><td>46.65</td></tr><tr><td><p>8</p><span class="date label label-default">Oct 25, 2018</span></td><td style="word-break:break-word;">SAN (single model)<p class="institution">Microsoft Business Applications Research Group</p><a class="link" href="https://arxiv.org/pdf/1712.03556.pdf">https://arxiv.org/pdf/1712.03556.pdf</a></td><td>39.77</td><td>40.72</td></tr><tr><td><p>9</p><span class="date label label-default">Oct 25, 2018</span></td><td style="word-break:break-word;">DocumentQA (single model)<p class="institution">JHU [modification of the AllenNLP implementation]</p><a class="link" href="https://arxiv.org/pdf/1710.10723.pdf">https://arxiv.org/pdf/1710.10723.pdf</a></td><td>38.52</td><td>39.76</td></tr><tr><td><p>10</p><span class="date label label-default">Oct 25, 2018</span></td><td style="word-break:break-word;">ASReader (single model)<p class="institution">JHU [modification of the IBM Waston implementation]</p><a class="link" href="https://arxiv.org/pdf/1603.01547.pdf">https://arxiv.org/pdf/1603.01547.pdf</a></td><td>29.80</td><td>30.35</td></tr><tr><td><p>11</p><span class="date label label-default">Oct 25, 2018</span></td><td style="word-break:break-word;">Random Guess<p class="institution">JHU</p></td><td>18.55</td><td>19.12</td></tr><tr><td><p>12</p><span class="date label label-default">Oct 25, 2018</span></td><td style="word-break:break-word;">Language Models (single model)<p class="institution">JHU [modification of the Google Brain implementation]</p><a class="link" href="https://arxiv.org/pdf/1806.02847.pdf">https://arxiv.org/pdf/1806.02847.pdf</a></td><td>17.57</td><td>18.15</td></tr></table></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="rightNav"><div><ul class="nav navbar-nav navbar-right"><li><a href="/ReCoRD-explorer/">Re<span class="record">Co</span>RD</a></li><li><a href="http://clsp.jhu.edu">CLSP@JHU</a></li></ul></div></div></div></nav><script src="/ReCoRD-explorer/bower_components/jquery/dist/jquery.min.js"></script><script src="/ReCoRD-explorer/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>